from "itf/options-itf-clifwrap.h" import *
from "cudamatrix/cu-matrixdim-clifwrap.h" import *
from "cudamatrix/cu-array-clifwrap.h" import *
from "cudamatrix/cu-matrix-clifwrap.h" import *
from "nnet3/nnet-computation-clifwrap.h" import *
from "rnnlm/rnnlm-example-clifwrap.h" import *

from "rnnlm/rnnlm-example-utils.h":
  namespace `kaldi::rnnlm`:
    def `RenumberRnnlmExample` as renumber_rnnlm_example(minibatch: RnnlmExample) -> list<int>:
      """Renumbers word-ids in a minibatch.

      This function renumbers the word-ids referred to in a minibatch, creating
      a numbering that covers exactly the words referred to in this minibatch.
      It is only to be called when sampling is used, i.e. when
      `minibatch.samples` is not empty.

      Args:
        minibatch (RnnlmExample): The minibatch to be modified. At entry the
          words-indexes in fields `input_words`, and `sampled_words` will be in
          their canonical numbering. At exit the numbers present in those
          arrays will be indexes into the `active_words` vector that this
          function outputs. For instance, suppose `minibatch.input_words[9] ==
          1034` at entry; at exit we might have `minibatch.input_words[9] ==
          94`, with `active_words[94] == 1034`. This function requires that
          `minibatch.sampled_words` is nonempty. If `minibatch.sampled_words`
          is empty, it means that sampling has not been done, so the negative
          part of the objf will use all the words. In this case the minibatch
          implicitly uses all words, so there is no use in renumbering. At
          exit, `minibatch.vocab_size` will have been set to the same value as
          `len(active_words)`.

      Returns:
        The list of active words, i.e. the words that were present in the
        fields `input_words`, and `sampled_words` in `minibatch` on entry. At
        exit, this list will be sorted and unique.

      Note:
          It is not necessary for this function to renumber `output_words`
          because in the sampling case they are indexes into blocks of
          `sampled_words` (see documentation for `RnnlmExample`).
      """

    def `GetRnnlmComputationRequest` as get_rnnlm_computation_request(
      minibatch: RnnlmExample, need_model_derivative: bool,
      need_input_derivative: bool, store_component_stats: bool) -> ComputationRequest:
      """Creates a computation request for the given RNNLM example.

      This function takes an RnnlmExample (which should already have been
      frame-selected, if desired, and merged into a minibatch) and produces a
      ComputationRequest. It assumes you don't want the derivatives w.r.t. the
      inputs; if you do, you can create/modify the ComputationRequest manually.
      Assumes that if `need_model_derivative` is true, you will be supplying
      derivatives w.r.t. all outputs.
      """

    class RnnlmExampleDerived:
      """Various quantities/expressions derived from an RNNLM example.

      This class contains various quantities/expressions that are derived from
      the quantities found in `RnnlmExample`, and which are needed when
      training on that example, particularly by the function
      :meth:`process_rnnlm_output`.
      """
      cu_input_words: CuArray
      """CUDA copy of minibatch.input_words."""

      cu_output_words: CuArray
      """CUDA copy of minibatch.output_words.

      It's only used in the sampling case.
      """

      cu_sampled_words: CuArray
      """CUDA copy of minibatch.sampled_words.

      It's only used in the sampling case (in the no-sampling case,
      minibatch.sampled_words would be empty anyway).
      """

      # FIXME: Need to wrap CuSparseMatrix first.
      #
      # output_words_smat: CuSparseMatrix
      # """The output words.
      #
      # This is a CuSparseMatrix constructed from `vocab_size`, `output_words`
      # and `output_weights` of an RnnlmExample that is only used in the
      # no-sampling case. Its num-rows is equal to `len(minibatch.output_words)`
      # and num-cols is equal to `minibatch.vocab_size`.
      # """
      #
      # input_words_smat: CuSparseMatrix
      # """The input words.
      #
      # This is a CuSparseMatrix used in backpropagating the derivatives w.r.t.
      # the input words back to the word-embedding. It is of dimension
      # `minibatch.vocab_size` by `len(minibatch.input_words)`, and is all zeros
      # except that it contains ones in positions `(minibatch.input_words[i],i)`.
      # """

      def `Swap` as swap(self, other: RnnlmExampleDerived):
        """Swaps contents with another derived RNNLM example.

        Args:
          other (RnnlmExampleDerived): The other derived RNNLM example.
        """

    def `GetRnnlmExampleDerived` as get_rnnlm_example_derived(
      minibatch: RnnlmExample, need_embedding_deriv: bool) -> RnnlmExampleDerived:
      """Constructs a derived RNNLM example.

      Sets up the structure containing derived parameters used in training
      and objective function computation.

      Args:
        minibatch (RnnlmExample): The input minibatch for which we are
          computing the derived parameters.
        need_embedding_deriv (bool): True if we are going to be computing
          derivatives w.r.t. the word embedding (e.g., needed in a typical
          training configuration); if this is True, it will compute
          `input_words_tranpose`.

      Returns:
        A derived RNNLM example structure for the input minibatch.
      """

    class RnnlmObjectiveOptions:
      """Options for RNNLM objective function.

      Configuration class relating to the objective function used for RNNLM
      training, more specifically for use by the function
      :meth:`process_rnnlm_output`.
      """
      den_term_limit: float
      """Modification to the with-sampling objective.

      This prevents instability early in training, but in the end makes no
      difference. We scale down the denominator part of the objective when
      the average denominator part of the objective, for this minibatch, is
      more negative than this value.  Set this to 0.0 to use unmodified
      objective function
      """
      max_logprob_elements: int
      """Maximum number of elements in the logprob matrix.

      Maximum number of elements when we allocate a matrix of size
      [minibatch-size, num-words] for computing logprobs of words. If the
      size is exceeded, we will break the matrix along the minibatch axis
      and compute them separately.
      """

      def `Register` as register(self, opts: OptionsItf):
        """Registers options with an object implementing the options interface.

        Args:
          opts (OptionsItf): An object implementing the options interface.
            Typically a command-line option parser.
        """

    def `ProcessRnnlmOutput` as process_rnnlm_output(
      objective_opts: RnnlmObjectiveOptions,
      minibatch: RnnlmExample, derived: RnnlmExampleDerived,
      word_embedding: CuMatrixBase, nnet_output: CuMatrixBase,
      word_embedding_deriv: CuMatrixBase, nnet_output_deriv: CuMatrixBase)
      -> (weight: float, objf_num: float, objf_den: float, objf_den_exact:float):
      """Processes the output of RNNLM computation.

      This function processes the output of the RNNLM computation for a single
      minibatch; it outputs the objective-function contributions from the
      numerator and denominator terms, and [if requested] the derivatives
      of the objective function w.r.t. the data inputs.

      In the explanation below, the index `i` encompasses both the time `t`
      and the member `n` within the minibatch.
      The objective function referred to here is of the form
        `objf = \\sum_i weight(i) * ( num_term(i) + den_term(i) )`
      where num_term(i) is the log-prob of the 'correct' word, which equals
      the dot product of the neural-network output with the word embedding,
      which we can write as follows
        `num_term(i) = l(i, minibatch.output_words(i))`
      where `l(i, w)` is the unnormalized log-prob of word `w` for position
      `i`, specifically
        `l(i, w) = vec_vec(nnet_output.Row(i), word_embedding.Row(w))`.
      Without importance sampling (if `len(minibatch.sampled_words) == 0`),
      we get
        `den_term(i) = 1.0 - (\\sum_w q(i,w))`
      This is a lower bound on the 'natural' normalizer term which is of the
      form `-log(\\sum_w p(i,w))`, and its linearity in the p's allows
      importance sampling). Here,
        `p(i, w) = exp(l(i, w))`

        `q(i, w) = exp(l(i, w)) if l(i, w < 0) else  1 + l(i, w)`
      [the reason we use `q(i, w)` instead of `p(i, w)` is that it gives a
      closer bound to the natural normalizer term and helps avoid
      instability in early phases of training.]

      With importance sampling (if minibatch.sampled_words.size() > 0),
      `den_term` equals
        `den_term(i) =  1.0 - (\\sum_w q(w,i) * sample_inv_prob(w,i))`
      where `sample_inv_prob(w, i)` is zero if word w was not sampled
      for this `t`, and 1.0 / (the probability with which it was sampled)
      if it was sampled.

      Args:
        objective_opts (RnnlmObjectiveOptions): Options for RNNLM objective.
        minibatch (RnnlmExample): The minibatch for which we are processing
          the output.
        derived (RnnlmExampleDerived): This struct contains certain quantities
          which are precomputed from `minibatch`. It's to be generated by
          calling :meth:`get_rnnlm_example_derived` prior to calling this
          function.
        word_embedding (CuMatrixBase): The word-embedding, dimension is
          num-words by embedding-dimension. This does not have to be 'real'
          word-indexes, it can be fake word-indexes renumbered to include only
          the required words if sampling is done; c.f.
          :meth:`renumber_rnnlm_example`.
        nnet_output (CuMatrixBase): The neural net output. Num-rows is
          `minibatch.chunk_length * minibatch.num_chunks`, where the stride for
          the time `0 <= t < chunk_length` is larger, so there are a block of
          rows for `t=0`, a block for `t=1`, and so on.  Num-columns is
          embedding-dimension.
        word_embedding_deriv (CuMatrixBase): If not None, the derivative of the
          objective function w.r.t. `word_embedding` is *added* to this
          location.
        nnet_output_deriv (CuMatrixBase): If not None, the derivative of the
          objective function w.r.t. `nnet_output` is *added* to this location.

      Returns:
        * **weight** -- The total weight over this minibatch. It is equal to
          `minibatch.output_weights.sum()`.
        * **objf_num** -- The total numerator part of the objective function,
          i.e. the sum over `i` of `weight(i) * num_term(i)`.
        * **objf_den** -- The total denominator part of the objective function,
          i.e. the sum over `i` of `weight(i) * den_term(i)`. You add this to
          `objf_num` to get the total objective function.
        * **objf_den_exact** -- If we're not doing sampling (i.e. if
          `len(minibatch.sampled_words) == 0`), the 'exact' denominator part of
          the objective function, i.e. the weighted sum of `exact_den_term(i) =
          -log(\\sum_w p(i,w))`. If we are sampling, then there is no exact
          denominator part, and this will be set to zero. This is provided for
          diagnostic purposes. Derivatives will be computed w.r.t. the
          objective consisting of `objf_num + objf_den`, i.e. ignoring the
          'exact' one.
      """
